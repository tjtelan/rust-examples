First attempt I tried to configure diesel and protos at the same time, but it ended up being too complicated.

Second attempt I got the gRPC communication working with the CLI, and then incorporating Diesel.

Documenting setup:
* Started postgres in docker
* `diesel setup`
* `diesel migration generate create_orders`
* Looked at the OrderRecord proto type to write `up.sql`. I think I can just use integer types, and use grpc code to convert to the native types.
* `diesel migration run` -- This generated `src/schema.rs`

I tried to manually modify shipment.rs, but it was immediately regenerated. Considering disabling the automatic build.rs stuff so I can modify it after compilation.

After more reading of the Diesel walkthrough, it appears that the proto structs may be unsuitable for Diesel. I fear that we will need to wrap some types around the protos. On the surface, that already sounds like a bit of double work.
- Design the protos
- Design the database tables
- Generate the proto models
- Wrap the proto models for Diesel #[derive()] annotations. Find out if I can just implement the traits? Hunch is no/not worth effort.
- Have conversions to/from the proto models to use w/ Diesel.

Is this maintainable? Going to say no.

---

Third attempt:

Started out with moving the codebase to use Cargo Workspaces. This took a moment but it is now done.

Diesel's getting started guide sets up an environment that requires a running database to infer the schema. This is a nice feature, but we don't want to use it. I commented out the contents of `diesel.toml`

Additionally, as a way to keep the diesel and gRPC code reasonably separate, I made them their own crates and added them as additional workspaces. Build warnings suggesting I compile client and backend separately have disappeared, because I am no longer running the protobuf build during client and server compilation.

> Good organization of these internal crates will go a long way to provide a pattern for adding new types of commands through the cli. New types that will care for the conversion to/from protobuf messages, and to/from the database.

I found this Stack Overflow post that sought out to do what I wanted with mapping a custom type into the schema definition: https://stackoverflow.com/questions/49092437/how-do-i-implement-queryable-and-insertable-for-custom-field-types-in-diesel

The [top post](https://stackoverflow.com/a/49097099) suggests implementing 2 traits: `ToSql` and `FromSql` and linked to the Diesel custom types test that shows a good model for implementing on a custom type.
https://github.com/diesel-rs/diesel/blob/v1.3.1/diesel_tests/tests/custom_types.rs

I went ahead and tried to adapt my database schema to the custom types model, and verified that I was able to insert on `cargo run`. Seemed to be working.

(BTW, I found this helpful mapping of Diesel to Postgres type conversions https://kotiri.com/2018/01/31/postgresql-diesel-rust-types.html.)

Now I was ready to adapt gRPC client/server code. Since I was a little unsure of what the end result would look like, I approached by playing with the proto types on the client-side. I would create the proto message type, populate it with data, and make the grpc service call. On the backend, I would create a new instance of my custom type, and directly convert the message data to use my custom enums.

The way I solved this problem was to create a rust type that mirrors the generated type, but only w/ the message fields and not the special fields. (It was because of these fields that I opted to not try and force Diesel to directly use the protobuf structs.)

What was left was the conversion between this my custom message type, and the proto message type. I implemented the `From` trait the proto type to custom type, and custom type to prototype. This is a nice compartmentalized area to map the enum types. On the backend side, I would still need to build a database query from my message, but since our my custom message can easily convert from proto to using common custom types with Diesel, it allows the code to be a lot cleaner. We have type safety from our client request, all the way to the db query.

This seems like a workable, scalable pattern.

client/
├── Cargo.toml
└── src
    ├── cli.yaml
    └── client.rs

This is the client-side command line. Simple cli to handle user input. We translate user input into the proper protobuf message structs, and make the unary rpc calls to the backend.

backend/
├── Cargo.toml
└── src
    ├── backend.rs
    └── backend.yaml

This is the server-side. Starts a gRPC server. We implement the RegistryService trait to handle the service endpoints. We translate incoming protobuf messages into our custom types, and translate back into protobuf when we sink the response to the client.

migrations/
├── 00000000000000_diesel_initial_setup
│   ├── down.sql
│   └── up.sql
└── 2019-03-18-213310_create_orders
    ├── down.sql
    └── up.sql

This directory was created with the `diesel` cli. The create_orders directory contains our starting point for the database, including custom Postgres enum types.

models/
├── Cargo.toml
├── client.rs
├── convert.rs
├── mod.rs
├── orders.rs
└── schema.rs

Models is a little complicated, since we're trying to take care of all of the fussy type switching within this crate for gRPC and Diesel usage.
* Schema is where we define our database schema w/ our custom rust types. In order to leverage Diesel's ability to convert to/from our Rust types and Postgres' types, we implement the ToSql and FromSql traits (for our rust enum), and leverage those traits to derive SqlType on Diesel's handler struct.
* Orders is where we keep Rust types that primarily were modeled for using w/ Diesel and our database.
* Convert contains the impl of the `From` traits for our Diesel Rust types, and our gRPC Rust types.
* Client is where helper functions for working with our models are implemented. Database operations are performed here

protos/
├── Cargo.toml
├── build.rs
├── mod.rs
├── refinery.proto
├── refinery.rs
└── refinery_grpc.rs

* build.rs is defines what protos we are building (refinery.proto), and when to build them.
* refinery.proto is the file we author for build.rs
* refinery_grpc.rs contains the generated boilerplate for the Service from refinery.proto. It generates a trait definition we implement in the backend crate, and other boilerplate for creating and starting the gRPC server.
* refinery.rs has all of the generated Rust code for protobuf message serialization, and struct CRUD operations that correspond to the protobuf message fields. Because the generated code contains extra elements that is needed for the gRPC generated traits, we are unable to reuse the generated code outside of the intended domain of serialization/deserialization for gRPC.
